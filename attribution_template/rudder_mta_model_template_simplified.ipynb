{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40db04b1",
   "metadata": {},
   "source": [
    "### Read Me:\n",
    "\n",
    "In this notebook, we calculate multi-touch attribution values for a given set of channels using following methods:\n",
    "1. Shapley values\n",
    "2. Markov chain values \n",
    "3. Last touch based \n",
    "\n",
    "* Shapley values code is implemented based on the logic presented in this [paper](https://arxiv.org/pdf/1804.05327.pdf)\n",
    "* Markov chain values are based on the following [whitepaper](https://www.channelattribution.net/pdf/Whitepaper.pdf)\n",
    "\n",
    "Setup:\n",
    "1. This code runs in AWS Sagemaker.\n",
    "2. Input data is to be prepared using Rudderstack Reverse ETL (more details in blog post linked at the end)\n",
    "3. Edit constants in the cell below. Only the first cell requires editing. No other edit is necessary. \n",
    "4. The output reslults get pushed to RudderStack event stream to a Python SDK source. From RudderStack website, we can setup a destination and push the data to any supported destination.\n",
    "5. The output results also get posted as a parquet file. The same can also be viewed in the notebook itself, as we go towards the end. \n",
    "6. Throughout the notebook, along with the code, we can also see several descriptions around the data and the results.\n",
    "7. Towards the end, we also have a robustness check to ensure how robust the results are if input data changes.\n",
    "8. **Max no:of touches permissible for shapley value model is around 15.** If larger than this, the algo fails as no:of combinations increases exponentially. We can either drop a few unimportant touches or group related touches together to reduce this count.\n",
    "\n",
    "Blog post about Rudderstack ETL and event stream used for the model: \n",
    "\n",
    "https://www.rudderstack.com/blog/from-first-touch-to-multi-touch-attribution-with-rudderstack-dbt-and-sagemaker/\n",
    "\n",
    "This notebook can run in any standard python environment.\n",
    "\n",
    "It is accompanied with a creds.yaml file that expects following keys:\n",
    "\n",
    "```\n",
    "rudder:\n",
    "  DATA_PLANE_URL: <from Rudderstack webapp>\n",
    "  WRITE_KEY: <from Rudderstack webapp>\n",
    "aws:\n",
    "  access_key_id: <from aws account>\n",
    "  access_key_secret: <from aws account>\n",
    "  region: <from aws account>\n",
    "```\n",
    "\n",
    "The aws credentials should point to an account that have access to the aws bucket location where RudderStack Warehouse actions pushes the data into.\n",
    "\n",
    "It also has a requirements.txt file that needs to be run for the first time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfede3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requirements.txt\n",
    "# This needs to be run only for the first time to install all required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb619080",
   "metadata": {},
   "source": [
    "### Part I: Constants:\n",
    "\n",
    "Define constants below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c826cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS DATA BUCKET where both input data is stored, and output data should be stored. \n",
    "# RudderStack Reverse ETL should be configured to write into this location\n",
    "S3_BUCKET = '<enter your s3 bucket name>'\n",
    "\n",
    "# INPUT DATA LOCATION. Folder within aws bucket where Rudderstack Reverse ETL pushes the input data into\n",
    "S3_INPUT_FOLDER_LOCATION = '<enter the folder path inside the s3>'\n",
    "\n",
    "# INPUT LABEL LOCATION. Folder within aws bucket where input label data is stored as json files (extracted from Rudderstack Reverse ETL)\n",
    "S3_LABEL_DATA_FOLDER_LOCATION = '<enter the folder path inside the s3>'\n",
    "\n",
    "# Output location. Folder within aws bucket where output data \n",
    "S3_OUTPUT_FOLDER_LOCATION='<enter the folder path inside the s3 where output should be copied to>'\n",
    "\n",
    "# MODEL CONFIGURATION.\n",
    "\n",
    "# We may want to combine a few touches into one group. Ex: all video ads from different sources may be combined into one.\n",
    "# In such case, we make below flag True (else, False) and create a touchpoint_mapping dictionary (see below) to group related touch points together\n",
    "GROUP_TOUCHES = False\n",
    "\n",
    "# This needs to be configured based on your touch points. \n",
    "# This is required only if GROUP_TOUCHES = True. In below mapping, each key corresponds to a touch we want to group. \n",
    "# Ex, sources, directory, home etc are all converted to 'webapp' and video-library, guides, case-studies are converted to 'docs'\n",
    "# If a touch is not found in the key, no modification is applied on that and it stays as is.\n",
    "\n",
    "touchpoint_mapping = {\"sources\": \"webapp\",\n",
    "                      \"directory\": \"webapp\",\n",
    "                      \"home\": \"webapp\",\n",
    "                      \"destinations\": \"webapp\",\n",
    "                      \"integration\": \"product\",\n",
    "                      \"syncs\": \"webapp\",\n",
    "                      \"transformations\": \"product\",\n",
    "                      \"team\": \"webapp\",\n",
    "                      \"video-library\": \"docs\",\n",
    "                      'case-studies': 'docs'}\n",
    "\n",
    "#touchpoint_mapping = None # If no mapping is required.\n",
    "\n",
    "# Dedup logic. IF same event repeats consecutively within this interval (in seconds), they are considered the same and first occurence timestamp is counted. \n",
    "# If we don't want a dedup logic, we can make this value as 0.\n",
    "MIN_EVENT_INTERVAL_IN_SEC = 300\n",
    "\n",
    "\n",
    "# Max no:of touches permissible for shapley value model. If larger than this, the algo fails as no:of combinations increases exponentially\n",
    "MAX_DISTINCT_TOUCHES=15\n",
    "\n",
    "## Data specific column variables. \n",
    "\n",
    "ENTITY_COL = 'domain'\n",
    "TIMESTAMP_COL  = 'timestamp'\n",
    "CONVERSION_TIME_COL = 'call_conversion_time'\n",
    "EVENT_COL = 'touch'\n",
    "ID_COL = 'row_id' # Random id used in Event Stream. \n",
    "\n",
    "\n",
    "# Rudder Analytics parameters (Ref: https://www.rudderstack.com/docs/stream-sources/rudderstack-sdk-integration-guides/rudderstack-python-sdk/)\n",
    "\n",
    "USER_ID = 'multitouch_attribution_notebook' \n",
    "EVENT_TYPE = 'multitouch_attribution_results'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e37c6b",
   "metadata": {},
   "source": [
    "**No other manual change is expected. Rest of the code should run without any edit.**\n",
    "\n",
    "You can run the whole notebook by going to `Run -> Run all cells`. This creates the parquet file output in prescribed s3 location, and also prints the data here for instant consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc29970b",
   "metadata": {},
   "source": [
    "### Part II: Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d550e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the required libraries\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "from typing import List, Optional, Union, Dict, Tuple\n",
    "import datetime\n",
    "\n",
    "from math import factorial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging\n",
    "import yaml\n",
    "import itertools\n",
    "from dataclasses import dataclass\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e9eef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    level=logging.INFO,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "formatter = logging.Formatter(fmt='%(asctime)s %(levelname)-8s %(message)s',\n",
    "                              datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "output_file_handler = logging.FileHandler(\"attribution_model.log\")\n",
    "output_file_handler.setFormatter(formatter)\n",
    "\n",
    "#stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "\n",
    "logger.addHandler(output_file_handler)\n",
    "#logger.addHandler(stdout_handler)\n",
    "\n",
    "# To prevent unwanted logs from matplotlib and pandas \n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "logging.getLogger('chardet.charsetprober').setLevel(logging.INFO)\n",
    "\n",
    "logging.info(\"\\n\\n\\t\\tSTARTING MTA MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136be1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_suffix = datetime.datetime.strftime(datetime.datetime.now(), '%Y-%m-%dT%H:%M:%S')\n",
    "s3_output_location = f\"s3://{S3_BUCKET}/{S3_OUTPUT_FOLDER_LOCATION}/{time_suffix}\"\n",
    "local_output_location = f\"data/mta_output_files/{time_suffix}\"\n",
    "os.makedirs(local_output_location)\n",
    "logging.info(f\"MTA values will be written to the location:\\n\\t{local_output_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e1d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper functions to load input data\n",
    "def load_json(json_str: str,\n",
    "              key: Optional[str]=None):\n",
    "    if key:\n",
    "        return json.loads(json_str)[key]\n",
    "    else:\n",
    "        return json.loads(json_str)\n",
    "\n",
    "def load_s3_compressed_dictionary_object(s3_file_object, key: Optional[str]=None):\n",
    "    with gzip.GzipFile(fileobj=s3_file_object.get()['Body']) as gzipfile:\n",
    "        content = gzipfile.read().decode('utf-8').split('\\n')\n",
    "    \n",
    "    return [load_json(line, key) for line in content if line]\n",
    "\n",
    "def load_json_data_from_s3(s3_instance, s3_bucket_name: str, files_prefix: str, json_dict_key: Optional[str]=None) -> pd.DataFrame:\n",
    "    bucket = s3_instance.Bucket(s3_bucket_name)\n",
    "    all_rows = []\n",
    "    for file_obj in tqdm(bucket.objects.filter(Prefix=files_prefix)):\n",
    "        all_rows.append(load_s3_compressed_dictionary_object(file_obj, json_dict_key))\n",
    "    \n",
    "    df = pd.DataFrame.from_records([row for rows in all_rows for row in rows])\n",
    "    df.columns = [col.lower() for col in list(df)]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed546ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"creds.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3e3177",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(\n",
    "    aws_access_key_id=config['aws']['access_key_id'],\n",
    "    aws_secret_access_key=config['aws']['access_key_secret'],\n",
    "    region_name=config['aws']['region']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d089bfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Loading data from Reverse ETL output into Python\n",
    "s3 = session.resource('s3')\n",
    "\n",
    "raw_data = load_json_data_from_s3(s3, S3_BUCKET, S3_INPUT_FOLDER_LOCATION, json_dict_key='traits')\n",
    "\n",
    "\n",
    "raw_data[TIMESTAMP_COL] = pd.to_datetime(raw_data[TIMESTAMP_COL]).dt.tz_localize(None)\n",
    "\n",
    "label_data = load_json_data_from_s3(s3, S3_BUCKET, S3_LABEL_DATA_FOLDER_LOCATION, json_dict_key='traits')\n",
    "label_data[CONVERSION_TIME_COL] = pd.to_datetime(label_data[CONVERSION_TIME_COL]).dt.tz_localize(None)\n",
    "domain_conversion_dates = label_data.set_index(ENTITY_COL).to_dict()[CONVERSION_TIME_COL]\n",
    "raw_data = raw_data.drop(columns=[ID_COL, CONVERSION_TIME_COL]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00a2593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations on the raw data. We apply the constraints defined in the constants cell above.\n",
    "\n",
    "def dedup_by_ts_delta(df: pd.DataFrame, primary_key: str, timestamp: str, event_type: str, max_lag: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ### Parameters\n",
    "    1. df : pd.DataFrame\n",
    "        - User touches dataframe. \n",
    "    2. primary_key : str\n",
    "        - column name of the column that contains user_id. \n",
    "    3. timestamp: str\n",
    "        - column name of the column that contains event timestamp\n",
    "    4. event_type: str\n",
    "        - column name of the column that contains event/touch data\n",
    "    5. max_lag: int\n",
    "        - max time (in sec) between consecutive events to be considered as duplicates. \n",
    "\n",
    "    ### Returns\n",
    "    - DataFrame after doing following steps\n",
    "    Based on primary key and event_type, it checks if two consecutive events occur within the max_lag time window. If so, they are considered same event and the latter event is dropped. \n",
    "    \"\"\"\n",
    "    if max_lag <= 0:\n",
    "        return df\n",
    "    df = df.sort_values(by=[primary_key, timestamp], ascending=True).reset_index(drop=True)\n",
    "    original_columns = df.columns\n",
    "    df[f\"prev_{primary_key}\"] = df[primary_key].shift()\n",
    "    df[f\"prev_{event_type}\"] = df[event_type].shift()\n",
    "    df[f\"prev_{timestamp}\"] = df[timestamp].shift()\n",
    "\n",
    "    def is_duplicate(row):\n",
    "        if pd.isnull(row[f\"prev_{primary_key}\"]) or pd.isnull(row[f\"prev_{event_type}\"]) or pd.isnull(row[f\"prev_{timestamp}\"]):\n",
    "            return False\n",
    "        elif row[primary_key] == row[f\"prev_{primary_key}\"] and row[event_type] == row[f\"prev_{event_type}\"] and (\n",
    "                row[timestamp] - row[f\"prev_{timestamp}\"]).total_seconds() <= max_lag:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    df[\"is_duplicate\"] = df.progress_apply(is_duplicate, axis=1)\n",
    "    return df.query(\"is_duplicate==False\")[original_columns].reset_index(drop=True)\n",
    "\n",
    "def process_raw_data(raw_data_df: pd.DataFrame,\n",
    "                     dedup_min_time: int,\n",
    "                     reduce_touches: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ### Parameters\n",
    "    1. raw_data_df : Raw data \n",
    "    2. ignore_touches: Ignores the touches present in this list. \n",
    "    3. min_date: Ignores events before this date\n",
    "    4. dedup_min_time: Time (in sec) between two events of same type. Events that repeat within this interval are combined as one (earlier timestamp is considered)\n",
    "    5. reduce_touches : Whether to combine touchpoints based on their logical groupings\n",
    "\n",
    "    ### Returns\n",
    "    - DataFrame after doing following steps\n",
    "    1. Groups tracks pages if reduce_touches flag is True\n",
    "    2. Deduplicates based on 5 min interval \n",
    "    3. Ignores touches based on ignore_touches list\n",
    "    4. √êrops events before the min_date timestamp.\n",
    "    \"\"\"\n",
    "    if reduce_touches:\n",
    "        raw_data_df[EVENT_COL] = raw_data_df[EVENT_COL].apply(lambda touch: touchpoint_mapping.get(touch, touch))\n",
    "    \n",
    "    dedup_data_df = (dedup_by_ts_delta(raw_data_df\n",
    "                                       .query(f\"~{EVENT_COL}.isnull()\",\n",
    "                                              engine='python')\n",
    "                                       .drop_duplicates(),\n",
    "                                       ENTITY_COL,\n",
    "                                       TIMESTAMP_COL, \n",
    "                                       EVENT_COL, \n",
    "                                       dedup_min_time)\n",
    "                     .filter([ENTITY_COL, EVENT_COL, TIMESTAMP_COL])\n",
    "                    )\n",
    "    return dedup_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b1b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "touch_data_filtered = process_raw_data(raw_data,  MIN_EVENT_INTERVAL_IN_SEC, GROUP_TOUCHES)\n",
    "del raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7c5daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the label data to events data\n",
    "touch_data_filtered['is_converted'] = touch_data_filtered[ENTITY_COL].apply(lambda domain: 1 if domain in domain_conversion_dates else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f7c3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data:\n",
    "touch_data_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1592d08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert len(touch_data_filtered[EVENT_COL].unique()) <= MAX_DISTINCT_TOUCHES\n",
    "except AssertionError:\n",
    "    logging.info(\"Max no:of touches higher than permissible\")\n",
    "    raise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6d816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_touchpoints = touch_data_filtered.query(\"is_converted==1\", engine='python')\n",
    "negative_touchpoints = touch_data_filtered.query(\"is_converted==0\", engine='python')\n",
    "\n",
    "print(\"Summary stats on converted and non converted journeys:\\n\")\n",
    "print(f\"Total rows in converted journeys: {len(positive_touchpoints)}\")\n",
    "print(f\"Distinct converted journeys: {len(positive_touchpoints['domain'].unique())}\")\n",
    "print(f\"Total rows in non-converted journeys: {len(negative_touchpoints)}\")\n",
    "print(f\"Distinct non-converted journeys: {len(negative_touchpoints['domain'].unique())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6da387",
   "metadata": {},
   "source": [
    "### Part III: Data distribution (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8b9365",
   "metadata": {},
   "source": [
    "A few stats on the converted journeys before going into the actual attribution problem:\n",
    "\n",
    "Note: This section is not required to calculate the attribution values. Instead, it is just to show the distribution of no:of events and no:of days before users convert. You can skip directly to [Part IV](#MTA-Calculations-begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48306a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_summary = positive_touchpoints.groupby([ENTITY_COL]).agg({TIMESTAMP_COL: \"min\", EVENT_COL:[\"size\", \"nunique\"]}).reset_index()\n",
    "conversion_summary.columns = [ENTITY_COL, TIMESTAMP_COL, \"n_events\", \"n_distinct_events\"]\n",
    "conversion_summary[\"days_to_convert\"] = conversion_summary.apply(lambda row: (domain_conversion_dates.get(row[ENTITY_COL]) - row[TIMESTAMP_COL]).days, axis=1)\n",
    "\n",
    "conversion_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d187c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=(16,6))\n",
    "sns.histplot(conversion_summary[\"n_events\"], bins=50, ax=axs[0])\n",
    "axs[0].set_title(\"No:of events before conversion\")\n",
    "axs[0].set_ylabel(\"Conversions\")\n",
    "axs[0].set_xlabel(\"Event count\");\n",
    "\n",
    "sns.histplot(conversion_summary[\"days_to_convert\"], bins=70, ax=axs[1])\n",
    "axs[1].set_title(\"Days to conversion\")\n",
    "axs[1].set_ylabel(\"Conversions\")\n",
    "axs[1].set_xlim([0,100])\n",
    "axs[1].set_xlabel(\"Days since first seen\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2f8104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks percentile counts at various levels. Can modify this list to get a different percentile value (ex: For 90th percentile, add 90 to the list.)\n",
    "percentile_points = [0,5, 25, 50, 75, 95, 99, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53d45e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Avg no:of touches before a user converts: {conversion_summary['n_events'].mean():.2f}; Median: {np.median(conversion_summary['n_events'])}\")\n",
    "print(\"\\nPercentiles for No:of touches:\")\n",
    "(pd.DataFrame.from_dict(\n",
    "    dict(zip(percentile_points, \n",
    "             np.percentile(conversion_summary['n_events'],\n",
    "                           percentile_points).round())),\n",
    "    orient='index', \n",
    "    columns=['n_events'])\n",
    " .reset_index().rename(columns={\"index\":\"percentile\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0944e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Avg days to convert: {conversion_summary['days_to_convert'].mean():.2f}; Median: {np.median(conversion_summary['days_to_convert'])}\")\n",
    "print(\"\\nPercentiles for No:of days to convert:\")\n",
    "(pd.DataFrame.from_dict(\n",
    "    dict(zip(percentile_points, \n",
    "             np.percentile(conversion_summary['days_to_convert'],\n",
    "                           percentile_points).round())),\n",
    "    orient='index', \n",
    "    columns=['n_days_to_convert'])\n",
    " .reset_index().rename(columns={\"index\": \"percentile\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f79e17",
   "metadata": {},
   "source": [
    "<a id='MTA-Calculations-begin'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eaae5f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Part IV: Shapley values calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdf79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper function to transform data to the required input form \n",
    "\n",
    "def collect_touchpoints(touchpoints_df: pd.DataFrame, \n",
    "                        primary_key: str=ENTITY_COL, \n",
    "                        ts_column: str=TIMESTAMP_COL,\n",
    "                        touchpoint_column: str=EVENT_COL) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transform dataframe with each touch as a row to a new dataframe where each journey has a single row with all touches as a list in chronological order\n",
    "    touchpoints_df: A dataframe with each row corresponding to a touch. \n",
    "    primary_key: Name of the column containing unique user identifier\n",
    "    ts_column: Name of column containing timestamp using which journeys are sorted chronologically\n",
    "    touchpoint_column: Name of column containing touch points. \n",
    "    \n",
    "    Returns:\n",
    "    A dataframe with two columns, first has primary key and second has the touchpoint_column\n",
    "    \"\"\"\n",
    "    return (touchpoints_df\n",
    "            .sort_values(by=[primary_key, ts_column], \n",
    "                         ascending=True)\n",
    "            .reset_index(drop=True)\n",
    "            .groupby(primary_key)[touchpoint_column]\n",
    "            .apply(list)\n",
    "            .reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bb308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "touchpoints_list_pos = collect_touchpoints(positive_touchpoints)\n",
    "\n",
    "touchpoints_list_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969c36bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Computing normalizing factor for marginal contribution\n",
    "shapley_weight = lambda p, s: (factorial(s)*factorial(p-s-1)/factorial(p))\n",
    "\n",
    "# Getting shapley value of a given channel and values mapping for channel subsets.\n",
    "def compute_shapley_values(v_values_map: dict, \n",
    "                            channel: str,\n",
    "                            n_channels: int) -> float:\n",
    "    # Initiating shap with marginal contribution from S = null subset\n",
    "    shap = 1/n_channels * v_values_map.get(channel, 0) \n",
    "    for subset_str, subset_contrib in v_values_map.items():\n",
    "        subset = subset_str.split(\",\")\n",
    "        if channel not in subset:\n",
    "            subset_union_channel = ','.join(sorted(subset + [channel]))\n",
    "            marginal_contrib = (v_values_map.get(subset_union_channel,0) -\n",
    "                                v_values_map.get(subset_str,0))\n",
    "            shap += shapley_weight(n_channels, len(subset)) * marginal_contrib\n",
    "    \n",
    "    return shap\n",
    "\n",
    "# Helper function to generate subsets for a given set of channels\n",
    "def generate_subsets(touchpoints: List[str]) -> List[List[str]]:\n",
    "    subset_list = []\n",
    "    for subset_size in range(len(touchpoints)):\n",
    "        for subset in itertools.combinations(touchpoints, subset_size + 1):\n",
    "            subset_list.append(list(sorted(subset)))\n",
    "    return subset_list\n",
    "\n",
    "# Computes the utility function value v(s) for given subset S, using contribution values for all subsets.\n",
    "def utility_function(touchpoint_set: List[str], \n",
    "                     contributions_mapping: dict) -> Union[int, float]:\n",
    "    subset_list = generate_subsets(touchpoint_set)\n",
    "    return sum([contributions_mapping.get(','.join(subset),0) for subset in subset_list])\n",
    "\n",
    "\n",
    "# Master function combining all the above functions to compute shapley values for each touchpoint from a list of journeys. \n",
    "def get_shapley_values(journeys_list: List[List[str]], \n",
    "                       contribs_list: List[Union[int, float]])->Dict[str, float]:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        journeys_list (List[List[str]]): List of journeys.\n",
    "         Each journey is a list of touchpoints..\n",
    "        contribs_list (List[Union[int, float]]): List of contributions corresponding to each journey in journeys_list.\n",
    "         Should have same length as journeys_list\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: A dictionary with key as channel/touchpoint, and Shapley value as its value \n",
    "    \"\"\"\n",
    "    flattened_journeys = [channel for journey in journeys_list for channel in set(journey)]\n",
    "    unique_channels = sorted(list(set(flattened_journeys)))\n",
    "    all_subsets = generate_subsets(unique_channels)\n",
    "    contrib_map = {}\n",
    "    for n, journey in enumerate(journeys_list):\n",
    "        journey_ = \",\".join(sorted(set(journey))) # Ensures deduplication and sorting of journeys\n",
    "        contrib_map[journey_] = contrib_map.get(journey_,0) + contribs_list[n]\n",
    "    v_values = {}\n",
    "    for subset in all_subsets:\n",
    "        v_values[\",\".join(subset)] = utility_function(subset, contrib_map)\n",
    "        \n",
    "    shapley_values = {}\n",
    "    for channel in unique_channels:\n",
    "        shapley_values[channel] = compute_shapley_values(v_values, \n",
    "                                                          channel, \n",
    "                                                          len(unique_channels))\n",
    "    return shapley_values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f34eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "touches_shapley_values = get_shapley_values(touchpoints_list_pos[EVENT_COL].values, [1] * len(touchpoints_list_pos))\n",
    "touches_shapley_values = pd.DataFrame.from_dict(touches_shapley_values, orient=\"index\").reset_index()\n",
    "touches_shapley_values.columns = [EVENT_COL, \"shap\"]\n",
    "touches_shapley_values = touches_shapley_values.sort_values(by= 'shap', ascending=False).reset_index(drop=True)\n",
    "plt.figure(figsize=(16,6))\n",
    "sns.barplot(x=\"shap\", y=EVENT_COL, data=touches_shapley_values, color=\"salmon\")\n",
    "plt.title(\"Shapley Values\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b158c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "touches_shapley_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e271e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "touches_shapley_values[\"shap_normalized\"] = touches_shapley_values[\"shap\"]/touches_shapley_values[\"shap\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13b98b1",
   "metadata": {},
   "source": [
    "### Part V: Markov Chain Values Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf91b52",
   "metadata": {},
   "source": [
    "Index of transition counts: 1st: source, last: destination. 2 to -1: same order as dict_touches_inv keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3440f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_transition_counts(journey_list: List[List[str]], \n",
    "                               distinct_touches_list: List[str], \n",
    "                               is_positive: bool):\n",
    "    if is_positive:\n",
    "        destination_idx = -1\n",
    "    else:\n",
    "        destination_idx = -2\n",
    "    transition_counts = np.zeros(((len(distinct_touches_list)+3), (len(distinct_touches_list)+3)))\n",
    "    for journey in journey_list:\n",
    "        transition_counts[0, (distinct_touches_list.index(journey[0])+1)] += 1 # First point in the path\n",
    "        for n, touch_point in enumerate(journey):\n",
    "            if n == len(journey) - 1:\n",
    "                # Reached last point\n",
    "                transition_counts[(distinct_touches_list.index(touch_point)+1), destination_idx] += 1\n",
    "                transition_counts[destination_idx, destination_idx]+=1\n",
    "            else:\n",
    "                transition_counts[(distinct_touches_list.index(touch_point)+1), (distinct_touches_list.index(journey[n+1]) + 1)] +=1\n",
    "    transition_labels = distinct_touches_list.copy()\n",
    "    transition_labels.insert(0, \"Start\")\n",
    "    transition_labels.extend([\"Dropoff\", \"Converted\"])\n",
    "    return transition_counts, transition_labels\n",
    "\n",
    "row_normalize_np_array = lambda transition_counts: transition_counts / transition_counts.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "def plot_transitions(transition_probabilities: np.array, labels: List[str], title=\"Transition Probabilities\", show_annotations=True):\n",
    "    ax = sns.heatmap(transition_probabilities,\n",
    "                     linewidths=0.5,\n",
    "                     robust=True, \n",
    "                     annot_kws={\"size\":8}, \n",
    "                     annot=show_annotations,\n",
    "                     fmt=\".2f\",\n",
    "                     cmap=\"YlGnBu\",\n",
    "                     xticklabels=labels,\n",
    "                     yticklabels=labels)\n",
    "    ax.tick_params(labelsize=10)\n",
    "    ax.figure.set_size_inches((16, 10))\n",
    "    ax.set_ylabel(\"Previous Step\")\n",
    "    ax.set_xlabel(\"Next Step\")\n",
    "    ax.set_title(title);\n",
    "\n",
    "\n",
    "def get_transition_probabilities(converted_touchpoints_list: List[List[int]], \n",
    "                                 dropoff_touchpoints_list: List[List[int]], \n",
    "                                 distinct_touches_list: List[str], \n",
    "                                 visualize=False) -> Tuple[np.array, List[str]]:\n",
    "    pos_transitions, _ = generate_transition_counts(converted_touchpoints_list, distinct_touches_list, is_positive=True)\n",
    "    neg_transitions, labels = generate_transition_counts(dropoff_touchpoints_list, distinct_touches_list, is_positive=False)\n",
    "    all_transitions = pos_transitions + neg_transitions\n",
    "    transition_probabilities = row_normalize_np_array(all_transitions)\n",
    "    if visualize:\n",
    "        plot_transitions(transition_probabilities, labels, show_annotations=True)\n",
    "    return transition_probabilities, labels\n",
    "\n",
    "def converge(transition_matrix, max_iters=200, verbose=True):\n",
    "    T_upd = transition_matrix\n",
    "    prev_T = transition_matrix\n",
    "    for i in range(max_iters):\n",
    "        T_upd = np.matmul(transition_matrix, prev_T)\n",
    "        if np.abs(T_upd - prev_T).max()<1e-5:\n",
    "            if verbose:\n",
    "                print(f\"{i} iters taken for convergence\")\n",
    "            return T_upd\n",
    "        prev_T = T_upd\n",
    "    if verbose:\n",
    "        print(f\"Max iters of {max_iters} reached before convergence. Exiting\")\n",
    "    return T_upd\n",
    "\n",
    "\n",
    "def get_removal_affects(transition_probs, labels, ignore_labels=[\"Start\", \"Dropoff\",\"Converted\"], default_conversion=1.):\n",
    "    removal_affect = {}\n",
    "    for n, label in enumerate(labels):\n",
    "        if label in ignore_labels:\n",
    "            continue\n",
    "        else:\n",
    "            drop_transition = transition_probs.copy()\n",
    "            drop_transition[n,:] = 0.\n",
    "            drop_transition[n,-2] = 1.\n",
    "            drop_transition_converged = converge(drop_transition, 500, False)\n",
    "            removal_affect[label] = default_conversion - drop_transition_converged[0,-1]\n",
    "    return removal_affect\n",
    "\n",
    "def get_markov_attribution(tp_list_positive: List[List[int]],\n",
    "                           tp_list_negative: List[List[int]], \n",
    "                           distinct_touches_list: List[str], \n",
    "                           visualize=False) -> Tuple[Dict[str, float], np.array]:\n",
    "    transition_probabilities, labels = get_transition_probabilities(tp_list_positive, tp_list_negative, distinct_touches_list, visualize=visualize)\n",
    "    transition_probabilities_converged = converge(transition_probabilities, max_iters=500, verbose=False)\n",
    "    removal_affects = get_removal_affects(transition_probabilities, labels, default_conversion=transition_probabilities_converged[0,-1])\n",
    "    total_conversions = len(tp_list_positive)\n",
    "    attributable_conversions = {}\n",
    "    total_weight = sum(removal_affects.values())\n",
    "    for tp, weight in removal_affects.items():\n",
    "        attributable_conversions[tp] = weight/total_weight * total_conversions\n",
    "    return attributable_conversions, transition_probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e9a098",
   "metadata": {},
   "outputs": [],
   "source": [
    "touchpoints_list_neg = collect_touchpoints(negative_touchpoints)\n",
    "all_touches = list(positive_touchpoints[EVENT_COL].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb814a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_attribution_values, transition_probabilities = get_markov_attribution(touchpoints_list_pos[EVENT_COL].values, \n",
    "                                                                             touchpoints_list_neg[EVENT_COL].values, \n",
    "                                                                             all_touches, \n",
    "                                                                             visualize=True)\n",
    "plt.savefig(f\"{local_output_location}/transition_probabilities_heatmap.png\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fe7e0d",
   "metadata": {},
   "source": [
    "In the above graphic, we can see the transition probabilities from each touch (Y-axis) to the next touch (X-axis). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33292946",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_transitions, labels = generate_transition_counts(touchpoints_list_pos[EVENT_COL].values, all_touches, is_positive=True)\n",
    "neg_transitions, labels = generate_transition_counts(touchpoints_list_neg[EVENT_COL].values, all_touches, is_positive=False)\n",
    "all_transitions = pos_transitions + neg_transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684b89ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs=plt.subplots(1,2, figsize=(18, 5))\n",
    "sns.set_style(\"white\")\n",
    "sns.barplot(x=labels[:-2], y=100*all_transitions[:-2, -2]/all_transitions[:-2, -2].sum(), ci=None, color=\"salmon\", ax=axs[0])\n",
    "    \n",
    "axs[0].set_xticklabels(labels[:-2],rotation=60)\n",
    "axs[0].set_ylabel(\"Percent\")\n",
    "axs[0].set_title(\"Distribution of last touch before dropoff\");\n",
    "\n",
    "sns.barplot(x=labels[:-2], y=100*all_transitions[:-2, -1]/all_transitions[:-2, -1].sum(), ci=None, color=\"salmon\", ax=axs[1])\n",
    "axs[1].set_xticklabels(labels[:-2],rotation=60)\n",
    "axs[1].set_ylabel(\"Percent\")\n",
    "axs[1].set_title(\"Distribution of last touch before convert\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7673f925",
   "metadata": {},
   "source": [
    "### Part VI: Results summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e293358",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_touch_based_mta = touchpoints_list_pos[EVENT_COL].apply(lambda event_list: event_list[-1]).value_counts().reset_index()\n",
    "last_touch_based_mta.columns = [EVENT_COL, 'last_touches']\n",
    "last_touch_based_mta['last_touches'] = last_touch_based_mta['last_touches']/last_touch_based_mta['last_touches'].sum()\n",
    "\n",
    "markov_attr_values_df = pd.DataFrame.from_dict(markov_attribution_values, orient=\"index\").reset_index()\n",
    "markov_attr_values_df.columns = [EVENT_COL, \"markov\"]\n",
    "markov_attr_values_df['markov'] = markov_attr_values_df['markov']/markov_attr_values_df['markov'].sum()\n",
    "mta_values = markov_attr_values_df.merge(touches_shapley_values.filter([EVENT_COL, \"shap_normalized\"]).rename(columns={\"shap_normalized\":\"shap\"}),\n",
    "                                         on=EVENT_COL, how=\"outer\").merge(last_touch_based_mta, on=EVENT_COL,how='outer').fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36bae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mta_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cce624",
   "metadata": {},
   "outputs": [],
   "source": [
    "mta_long = pd.melt(mta_values, EVENT_COL, [\"markov\",  \"shap\", \"last_touches\"])\n",
    "plt.figure(figsize=(16,6))\n",
    "sns.barplot(data=mta_long, x=EVENT_COL,y='value',hue='variable');\n",
    "plt.title(\"Normalized values of various attribution methods\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.savefig(f\"{local_output_location}/mta_values.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c21f70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"Correlation between various values. Closer to 1 indicate they are giving same information. \\\n",
    "Closer to 0 indicate they are highly independent and negative values indicate they are giving opposing information\\n\\n\"\"\")\n",
    "mta_values.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b56d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mta_values.to_parquet(f\"{local_output_location}/mta_values.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41db0d0",
   "metadata": {},
   "source": [
    "### Part VII: Robustness testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27e6795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332eb79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_vals = {}\n",
    "shapley_vals = {}\n",
    "\n",
    "touchpoints_list_pos_rand1, touchpoints_list_pos_rand2 = train_test_split(touchpoints_list_pos, train_size=0.5)\n",
    "touchpoints_list_neg_rand1, touchpoints_list_neg_rand2 = train_test_split(touchpoints_list_neg, train_size=0.5)\n",
    "\n",
    "touches_shapley_values_rand1 = get_shapley_values(touchpoints_list_pos_rand1[EVENT_COL].values, [1] * len(touchpoints_list_pos_rand1))\n",
    "# get_shapley_values(touchpoints_list_pos_rand1[\"event_id\"].values, dict_touches, visualize=False)\n",
    "markov_attribution_values_rand1, _ = get_markov_attribution(touchpoints_list_pos_rand1[EVENT_COL].values, touchpoints_list_neg_rand1[EVENT_COL].values, all_touches, visualize=False)\n",
    "\n",
    "touches_shapley_values_rand2 = get_shapley_values(touchpoints_list_pos_rand2[EVENT_COL].values, [1] * len(touchpoints_list_pos_rand2))\n",
    "\n",
    "markov_attribution_values_rand2, _ = get_markov_attribution(touchpoints_list_pos_rand2[EVENT_COL].values, touchpoints_list_neg_rand2[EVENT_COL].values, all_touches, visualize=False)\n",
    "\n",
    "\n",
    "for key, val in touches_shapley_values_rand1.items():\n",
    "    shapley_vals[key] = [val, touches_shapley_values_rand2.get(key,0)]\n",
    "\n",
    "for key, val in markov_attribution_values_rand1.items():\n",
    "    markov_vals[key] = [val, markov_attribution_values_rand2.get(key,0)]\n",
    "\n",
    "    \n",
    "shapley_vals_df = pd.DataFrame.from_dict(shapley_vals, orient='index')\n",
    "markov_vals_df = pd.DataFrame.from_dict(markov_vals, orient='index')\n",
    "logging.info(\"Robustness testing:\")\n",
    "logging.info(f\"Correlation of shapley values between two non-overlapping splits: {shapley_vals_df.corr()[0][1]:.3f}\")\n",
    "logging.info(f\"Correlation of markov values between two non-overlapping splits: {markov_vals_df.corr()[0][1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eb0a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapley_vals_df = pd.melt(shapley_vals_df.reset_index(), id_vars='index')\n",
    "shapley_vals_df.columns = [EVENT_COL, 'iter', 'shap']\n",
    "\n",
    "markov_vals_df = pd.melt(markov_vals_df.reset_index(), id_vars='index')\n",
    "markov_vals_df.columns = [EVENT_COL, 'iter', 'markov']\n",
    "\n",
    "tp_order = shapley_vals_df.groupby(EVENT_COL)['shap'].mean().reset_index().sort_values(\"shap\", ascending=False)[EVENT_COL].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb89f8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(16,6))\n",
    "sns.barplot(x=EVENT_COL, y='shap', hue='iter', data=shapley_vals_df, ax=axs[0], order=tp_order)\n",
    "for item in axs[0].get_xticklabels():\n",
    "    item.set_rotation(90)\n",
    "axs[0].set_title(\"Shapley vals for touchpoints in non-overlapping splits\");\n",
    "\n",
    "sns.barplot(x=EVENT_COL, y='markov', hue='iter', data=markov_vals_df, ax=axs[1], order=tp_order)\n",
    "for item in axs[1].get_xticklabels():\n",
    "    item.set_rotation(90)\n",
    "axs[1].set_title(\"Markov vals for touchpoints in non-overlapping splits\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e58ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_vals = {}\n",
    "shapley_vals = {}\n",
    "for iters in range(10):\n",
    "    touchpoints_list_pos_rand, _ = train_test_split(touchpoints_list_pos, train_size=0.7)\n",
    "    touchpoints_list_neg_rand, _ = train_test_split(touchpoints_list_neg, train_size=0.7)\n",
    "    touches_shapley_values_rand = get_shapley_values(touchpoints_list_pos_rand[EVENT_COL].values, [1] * len(touchpoints_list_pos_rand))\n",
    "    markov_attribution_values_rand, _ = (get_markov_attribution(touchpoints_list_pos_rand[EVENT_COL].values, \n",
    "                                                                touchpoints_list_neg_rand[EVENT_COL].values,\n",
    "                                                                all_touches, \n",
    "                                                                visualize=False))\n",
    "    for touch, shap in touches_shapley_values_rand.items():\n",
    "        curr = shapley_vals.get(touch, [])\n",
    "        curr.append(shap)\n",
    "        shapley_vals[touch] = curr\n",
    "    for touch, mark in markov_attribution_values_rand.items():\n",
    "        curr = markov_vals.get(touch, [])\n",
    "        curr.append(mark)\n",
    "        markov_vals[touch] = curr\n",
    "\n",
    "shapley_ranks = pd.DataFrame.from_dict(shapley_vals, orient='index').fillna(0).rank(axis=0, ascending=False).astype(int)\n",
    "markov_ranks = pd.DataFrame.from_dict(markov_vals, orient='index').fillna(0).rank(axis=0, ascending=False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f385e33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(shapley_ranks.sort_values(by=0),\n",
    "                 linewidths=0.5,\n",
    "                 robust=True, \n",
    "                 annot_kws={\"size\":10}, \n",
    "                 annot=True,\n",
    "                 fmt=\"d\",\n",
    "                 cmap=\"YlGnBu\",\n",
    "                 cbar=False)\n",
    "ax.tick_params(labelsize=14)\n",
    "ax.figure.set_size_inches((10, 6))\n",
    "#ax.set_ylabel(\"Year\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_title(\"Shapley value rank in different iterations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344d69d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(markov_ranks.sort_values(by=0),\n",
    "                 linewidths=0.5,\n",
    "                 robust=True, \n",
    "                 annot_kws={\"size\":10}, \n",
    "                 annot=True,\n",
    "                 fmt=\"d\",\n",
    "                 cmap=\"YlGnBu\",\n",
    "                 cbar=False)\n",
    "ax.tick_params(labelsize=14)\n",
    "ax.figure.set_size_inches((10, 6))\n",
    "#ax.set_ylabel(\"Year\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_title(\"Markov Chain based value rank in different iterations\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873abcc7",
   "metadata": {},
   "source": [
    "Shapley values and markov values rank order remain quite stable with only minor differences, as can be seen above\n",
    "\n",
    "Markov is more robust compared to Shapley"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a224fdc",
   "metadata": {},
   "source": [
    "### Part VIII: Streaming records to Rudder Event Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dd20c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rudder_analytics\n",
    "import logging\n",
    "\n",
    "\n",
    "def np_decoder(element):\n",
    "    if isinstance(element, np.generic):\n",
    "        return element.item()\n",
    "    else:\n",
    "        return element\n",
    "    \n",
    "\n",
    "def stream(user_id, event_type, write_key, data_plane_url, df):\n",
    "    rudder_analytics.write_key = write_key\n",
    "    rudder_analytics.data_plane_url = data_plane_url\n",
    "    record_count = 0\n",
    "    for i in df.index:\n",
    "        event = {}\n",
    "        for j in df.columns:\n",
    "            event[j] = np_decoder(df.iloc[i][j])\n",
    "\n",
    "        rudder_analytics.track(user_id, event_type, event)\n",
    "        record_count = record_count + 1\n",
    "    rudder_analytics.flush()    \n",
    "    logging.info(f\"Total {record_count} records inserted into rudder_analytics as {event_type} event\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd0ee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream(USER_ID, EVENT_TYPE, config['rudder']['WRITE_KEY'], config['rudder']['DATA_PLANE_URL'], mta_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55d9e8b",
   "metadata": {},
   "source": [
    "### Part IX: Copying all results to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82992a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = session.client('s3')\n",
    "\n",
    "for root, dirs, files in os.walk(f'data/mta_output_files/{time_suffix}'):\n",
    "    for file in files:\n",
    "        s3_client.upload_file(os.path.join(root, file), S3_BUCKET, f\"{S3_OUTPUT_FOLDER_LOCATION}/{time_suffix}/{file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22be60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f\"Done generating mta values and streaming them to RS event-stream. All the output files are saved to s3 location  {S3_BUCKET}/{S3_OUTPUT_FOLDER_LOCATION}/{time_suffix}/{file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7969f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
